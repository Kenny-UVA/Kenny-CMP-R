---
title: "Kenny's CMP Portofolio"
author: "Kenny Wout"
date: "2023-02-24"
output: 
  flexdashboard::flex_dashboard:
    storyboard: TRUE
    theme: spacelab
---

### Introduction

<p>

<strong>Kenny's Computational Musicology Portofolio Repository</strong>

<p>

<p>

For my corpus I'll be looking at the music catalog of the three top best selling female R&B musicians, namely Beyoncé, Mariah Carey and Rihanna. They share a lot of similarities but also some compelling differences, such as the records they've broken, their cultural backgrounds, vocal capabilities and the genres that influence their music.

This means that there are many interesting angles that could be investigated such as how Mariah and Rihanna have way more no 1 hits in their career than Beyoncé. In contrast Beyoncé has 32 grammy's to her name which is the all-time record for most grammy wins, Rihanna has 9 awards while Mariah has 5. Mariah and Beyoncé are also widely regarded to be much better vocalists than Rihanna, but Rihanna is the one who has sold the most records out of the three of them. There are very peculiar differences in the career milestones they've achieved and I want to see if those differences can be explained by comparing their music, and find out if they're really all that different from each-other or actually the same.

I think my corpus is representative of the artists I want to compare, one weakness would be that at certain points of her career Rihanna's live vocals were not as of equal quality to her studio versions, but those live vocals are also not available on Spotify. I think my other challenge also lies in the fact that Mariah has started her career before Rihanna and Beyoncé and thus also has music that is typical of certain eras that Rihanna and Beyoncé don't, I also see this as an opportunity since some of Mariah's older Christmas album still manage to chart every decade so while it is a challenge it also presents interesting opportunities for comparison. All I want for Christmas by Mariah is thus also one her most interesting tracks which became a number 1 hit in 2019 25 years after it was released. Rihanna with songs like S&M and Rude Boy which have very controversial lyrics and don't seem to be specifically tailored for a massive audience but are still songs that still managed to peak at No. 1. Finally Beyoncé who has a lot of hits that champion female empowerment.

All wildly different strategies that seem to work but I want to find out if the music itself is really all that different. Another interesting aspect is comparing their fanbases to see how much overlap there is between them.

<p>

### Grammy's 

```{r ggplot}

library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
# library(conflicted)
library(dataset)


bmr <- 
get_playlist_audio_features('','2uzoyXBjDGEEpwAEMVfuB3')%>% 
 unnest(track.artists) %>%
  filter(name == "Beyoncé" | name == "Rihanna" | name == "Mariah Carey")

grammy <- 
  get_playlist_audio_features('','4lPqkYWl4aQEFUwRQWngHu') %>% 
  unnest(track.artists) %>%
  filter(name == "Beyoncé" | name == "Rihanna" | name == "Mariah Carey")

  grammy %>% ggplot(
    aes(
      x = acousticness,
      y = valence,
      colour = track.name,
      size = track.popularity 
    )
  ) +
  geom_point(aes()) +
  theme_minimal() +
 
  labs(
    
    x = "acousticness",
    
    y = "Valence",
    
    colour = "Track Name",
    
    size = "Popularity"
    )

print(ggplotly())
```

***

<p>

From my corpus I selected songs that have won a grammy. I did this because I wanted to what makes an excellent song, at least according to the grammy’s. And since the artists in my corpus are R&B musicians I’m expecting them to be more on the acoustic since R&B is an older genre and also low valance since since R&B usually also covers more serious subject matters. 


The results were unexpected but do however make sense if you consider how R&B has evolved to be more electronic. There are some outliers though like Black Parade which to me does not sound like it should have such low valance or We Belong together that has high valance but is actually a somewhat sad song it does sound a bit more upbeat though. 

<p>

### Hits {data-commentary-width="400"}
```{r ggplot556}

library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
# library(conflicted)
library(dataset)



hits <- 
  get_playlist_audio_features('','6DE69JDxkRWxQpkB5TNrBR') %>% 
  unnest(track.artists) %>%
  filter(name == "Beyoncé" | name == "Rihanna" | name == "Mariah Carey")

  hits %>% ggplot(
    aes(
      x = acousticness,
      y = valence,
      colour = name,
      size = track.popularity 
    )
  ) +
  geom_point(aes()) +
  theme_minimal() +
 
  labs(
    
    x = "Acousticness",
    
    y = "Valence",
    
    colour = "Artist",
    
    size = "Popularity"
    )

print(ggplotly())
```
***
```{r ggplot565}
hits %>% ggplot(
    aes(
      x = danceability,
      y = valence,
      colour = name,
      size = track.popularity 
    )
  ) +
  geom_point(aes()) +
  theme_minimal() +
 
  labs(
    
    x = "Danceability",
    
    y = "Valence",
    
    colour = "Artist",
    
    size = "Popularity"
    )

print(ggplotly())
```

***

<p>

From my corpus I also selected a few songs that reached no 1 on the US billboard charts, To see wether there's a difference between them and the grammy winning songs. I first looked at acousticness and valance just like before and then also danceability since I'm expecting these to be more danceable than the grammy winning tracks.



The results here again are interesting and we see that Mariah has the most number 1's but also that in terms of popularity some of those songs scored very low, that ofcourse most likely has to do with the fact that these are probably some of her older songs. Mariah is also the one who has scored the most  high on acousticness this again I think showcases how the genre has changed over the years. As expected most songs that have high danceability also tend to score high on valance. It's interesting that Mariah seems to be the only one who has managed to get multiple number 1's with songs that have a low valance rating and low danceability 

<p>

### Self Similarity Matrice - Black Parade 
```{r ggplot67}

library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
library(dataset)

blackp <-
  get_tidy_audio_analysis("2qzUpSVI4NnPyWxbXwumTj") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

blackp |>
  compmus_self_similarity(pitches, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

blackp |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

print(ggplotly())

```
***

<p>

From the previous experiments I selected Black Parade as an outlier since it was a song with a very Low Valance rating but also very low acousticness rating I want to see if there's anything in the structure of the song that would make spottifiy think this since to me this song should have scored higher on both

The chroma based self similarity matrice I did expect to be more vague since she is "rap singing" for the majority of the track There is something happening very clearly at around the 60 and I think this is the point in the track where she starts singing and thus changes in pitch pretty dramatically. 

With the timbre based self similarity matrice you can definetly see more of a strcuture but It's stil pretty vague, the track does have non-western influences which could potentially account for that but also at  the same time I think it also means that even though there is a lot of instrumentation and especially because said instrumentation is probably electronic, the beat does not change much and stays exactly the same for most of the track which would make sense for a modern R&B song. 



<p>


### Self Similarity Matrice - We Belong Together 
```{r ggplot97}

library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
library(dataset)

webelongt <-
  get_tidy_audio_analysis("4EI8VuxUuIHKfafU72emqz") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

webelongt |>
  compmus_self_similarity(pitches, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
  
print(ggplotly())


webelongt |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
  
print(ggplotly())

```


***

<p>

From the previous experiments I also selected We belong Together as an outlier since it sounds upbeat but still is very sad and wanted to see again here what the  strcuture of the song looks like. 

The chroma bases self similarity matrice is interesting because it does show a lot of pitch changes and quite rapid after eachother too, I think maybe this is a reason for why the track sounds more upbeat. I think this also showcases the melismatic nature of the way Mariah Carey sings her songs. 

The timbre based self similarity matrice is  also interesting because it kinds shows how the song is kind of this build up of energy until it reaches the climax at the end where she not only belts in full power but also the texture in her voice. I think it's interesting that spotify's timbre feautures were able to pick up on that. 


<p>

### Chordogram Single Ladies 

```{r ggplot2}

library(tidyverse)
library(spotifyr)
library(compmus)
library(conflicted)
library(plotly)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )


singleladies <-
  get_tidy_audio_analysis("557un1HgwYMuqfWGSTmnxw") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )
singleladies |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")


```

***

<p>

Single Ladies was also another outlier in my corpus because it is a hit and it also scored low on valance, but I think most would say that it is a really positive song. From what we've seen so far there is a trend with Beyoncé scoring lower on valance eeven if the track itself is not as negative. So now I'm wondering if they keys have something to do with it. 

From the chrodogram the first thing I noticed was how it showcases the way Beyoncé has a tendency of layering her vocals on top of eachother essentially harmonizing with herself, that is very apparent here. I was expecting to find more minor notes as a way of explaining the low valance, but it seems fairly well-balanced. The chordogram fails to give me much insight to give an answer to my question but is still intersting nonetheless. 

<p>


### Grammy's - Histogram {data-commentary-width="400"}

```{r ggplot5}
library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
# library(conflicted)
library(dataset)


renny <- get_playlist_audio_features('','4lPqkYWl4aQEFUwRQWngHu') 
 renny |> ggplot(aes(x = tempo, color = , fill = track.name)) + geom_histogram(binwidth = 5)
 
 print(ggplotly())
```

------------------------------------------------------------------------

<p>



<p>

### Black Parade - Tempogram {data-commentary-width="400"}

```{r ggplot6}

library(tidyverse)
library(spotifyr)
library(compmus)

summer_renaissance <- get_tidy_audio_analysis("2qzUpSVI4NnPyWxbXwumTj")
summer_renaissance |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***

<p>

I chose Black Parade again to do a tempogram of because I'm still kind of in awe of how a track that feels so unstructured can be so standardized and here again we see that the tempo barely changes over the entire song except for a small part in thee beginning. 

<p>

### Clustering 
```{r ggplot689}


```

### Conclusion

In Conclusion, I set out to explore what makes a good R&B song or rather what strategies do Mariah Carey, Beyoncé and Rihanna all employ that make them succesful R&B artists and I definetely gained some valuable insight 

